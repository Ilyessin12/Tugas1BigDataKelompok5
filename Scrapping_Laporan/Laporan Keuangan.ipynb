{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "import os\n",
    "import zipfile\n",
    "import xmltodict\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import tempfile\n",
    "import pymongo\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Scrapping data laporan keuangan dari IDX\n",
    "Langkah yang dilakukan :\n",
    "1. Automasi web menggunakan selenium untuk mendownload file instance.zip di folder downloads (akan membuat folder baru jika belum ada)\n",
    "2. extract zip file lalu parsing file taxonomy dengan xml\n",
    "3. setelah berhasil di parsing, output akan disimpan di financial_reports.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Konfigurasi Paths ===\n",
    "CURRENT_DIR = os.getcwd()\n",
    "BASE_DIR = os.path.join(CURRENT_DIR, \"downloads\")\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "# Location to save the final JSON data\n",
    "JSON_OUTPUT_FILE = os.path.join(CURRENT_DIR, \"financial_reports.json\")\n",
    "\n",
    "# Load existing data if file exists\n",
    "existing_reports = []\n",
    "already_scraped_companies = set()\n",
    "if os.path.exists(JSON_OUTPUT_FILE):\n",
    "    try:\n",
    "        with open(JSON_OUTPUT_FILE, 'r') as f:\n",
    "            existing_reports = json.load(f)\n",
    "            print(f\"Loaded {len(existing_reports)} existing reports from {JSON_OUTPUT_FILE}\")\n",
    "            # Create a set of companies that have already been scraped\n",
    "            already_scraped_companies = {report['company'] for report in existing_reports}\n",
    "            print(f\"Already scraped {len(already_scraped_companies)} companies\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error loading {JSON_OUTPUT_FILE}, will create a new one\")\n",
    "        existing_reports = []\n",
    "\n",
    "# === Konfigurasi Selenium WebDriver ===\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": BASE_DIR,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"safebrowsing.enabled\": True\n",
    "})\n",
    "options.add_experimental_option(\"detach\", True)  # Keep browser open\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# === URL IDX Laporan Keuangan ===\n",
    "url = \"https://www.idx.co.id/id/perusahaan-tercatat/laporan-keuangan-dan-tahunan\"\n",
    "\n",
    "# open the emiten_list.json file\n",
    "with open('emiten_list.json') as f:\n",
    "    data = json.load(f)\n",
    "    company_codes = [company_code.split(\".\")[0] for company_code in data]\n",
    "\n",
    "# Filter out companies that have already been scraped\n",
    "companies_to_scrape = [company for company in company_codes if company not in already_scraped_companies]\n",
    "print(f\"Will scrape {len(companies_to_scrape)} remaining companies out of {len(company_codes)} total\")\n",
    "\n",
    "# === Fungsi Konversi XML ke Dictionary ===\n",
    "def xml_to_dict(element):\n",
    "    \"\"\"Mengubah XML menjadi dictionary secara rekursif\"\"\"\n",
    "    data = {}\n",
    "    for child in element:\n",
    "        tag = child.tag.split(\"}\")[-1]  # Hapus namespace\n",
    "        if len(child) > 0:\n",
    "            data[tag] = xml_to_dict(child)\n",
    "        else:\n",
    "            data[tag] = child.text\n",
    "    return data\n",
    "\n",
    "# === Fungsi Parsing Taxonomy (.xsd) ===\n",
    "def parse_taxonomy(xsd_path):\n",
    "    \"\"\"Parsing file Taxonomy (.xsd) untuk mendapatkan definisi elemen\"\"\"\n",
    "    if not os.path.exists(xsd_path):\n",
    "        print(f\"❌ Taxonomy file {xsd_path} not found!\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(xsd_path)\n",
    "        root = tree.getroot()\n",
    "        taxonomy_dict = {}\n",
    "\n",
    "        for elem in root.iter():\n",
    "            tag = elem.tag.split(\"}\")[-1]  # Hapus namespace\n",
    "            if \"name\" in elem.attrib:\n",
    "                taxonomy_dict[elem.attrib[\"name\"]] = {\n",
    "                    \"type\": elem.attrib.get(\"type\", \"unknown\"),\n",
    "                    \"documentation\": elem.attrib.get(\"documentation\", \"No description\")\n",
    "                }\n",
    "\n",
    "        print(f\"✅ Parsed taxonomy {xsd_path}\")\n",
    "        return taxonomy_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error parsing taxonomy: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Dictionary to store all financial reports data, initialize with existing data\n",
    "all_financial_reports = existing_reports\n",
    "\n",
    "# === Proses Scraping, Download, Ekstraksi, Parsing, dan Simpan ke JSON ===\n",
    "for company in companies_to_scrape:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # Input kode perusahaan\n",
    "        search_box = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"input.vs__search\")))\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(company)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Pilih hasil pertama\n",
    "        first_suggestion = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".vs__dropdown-option\")))\n",
    "        first_suggestion.click()\n",
    "\n",
    "        # Pilih filter laporan keuangan, saham, tahun 2024, tahunan\n",
    "        driver.find_element(By.ID, \"FinancialStatement\").click()\n",
    "        driver.find_element(By.ID, \"TypeSaham\").click()\n",
    "        driver.find_element(By.ID, \"year1\").click()\n",
    "        driver.find_element(By.ID, \"period3\").click()\n",
    "        driver.find_element(By.XPATH, \"//button[contains(text(), 'Terapkan')]\").click()\n",
    "\n",
    "        # Tunggu tabel laporan muncul\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "\n",
    "        # Cari dan download file \"instance.zip\"\n",
    "        rows = driver.find_elements(By.TAG_NAME, \"tr\")\n",
    "        for row in rows:\n",
    "            if \"instance.zip\" in row.text:\n",
    "                link_element = row.find_element(By.TAG_NAME, \"a\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", link_element)\n",
    "                driver.execute_script(\"arguments[0].click();\", link_element)\n",
    "                print(f\"✅ Download started for {company}\")\n",
    "                break\n",
    "\n",
    "        # Tunggu beberapa detik agar download selesai\n",
    "        time.sleep(5)\n",
    "\n",
    "        # === Ekstraksi File ZIP ===\n",
    "        zip_path = os.path.join(BASE_DIR, \"instance.zip\")\n",
    "        extract_dir = os.path.join(BASE_DIR, company)\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(zip_path):\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(extract_dir)\n",
    "                print(f\"✅ Extracted {company} instance.zip\")\n",
    "            except zipfile.BadZipFile:\n",
    "                print(f\"❌ Error: {company} instance.zip is not a valid ZIP file\")\n",
    "                continue  # Lewati perusahaan ini jika ZIP rusak\n",
    "            finally:\n",
    "                os.remove(zip_path)  # Hapus ZIP setelah ekstraksi\n",
    "\n",
    "        # === Parsing Taxonomy ===\n",
    "        xsd_path = os.path.join(extract_dir, \"taxonomy.xsd\")\n",
    "        taxonomy_dict = parse_taxonomy(xsd_path)\n",
    "\n",
    "        # === Konversi XBRL ke JSON & Simpan ke Dictionary ===\n",
    "        xbrl_path = os.path.join(extract_dir, \"instance.xbrl\")\n",
    "\n",
    "        if os.path.exists(xbrl_path):\n",
    "            try:\n",
    "                tree = ET.parse(xbrl_path)\n",
    "                root = tree.getroot()\n",
    "                xbrl_dict = xml_to_dict(root)\n",
    "\n",
    "                # Gabungkan XBRL dengan Taxonomy\n",
    "                enriched_data = {\n",
    "                    \"company\": company,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"taxonomy\": taxonomy_dict,\n",
    "                    \"xbrl_data\": xbrl_dict\n",
    "                }\n",
    "\n",
    "                # Add to our collection of all data\n",
    "                all_financial_reports.append(enriched_data)\n",
    "                print(f\"✅ {company} data added to collection!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {company}.xbrl: {e}\")\n",
    "        else:\n",
    "            print(f\"❌ instance.xbrl not found for {company}\")\n",
    "\n",
    "        if len(all_financial_reports) % 5 == 0:\n",
    "            with open(JSON_OUTPUT_FILE, 'w') as f:\n",
    "                json.dump(all_financial_reports, f, indent=2)\n",
    "            print(f\"Interim save: {len(all_financial_reports)} reports saved to {JSON_OUTPUT_FILE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {company}: {e}\")\n",
    "        # Save progress after any error to ensure we don't lose data\n",
    "        with open(JSON_OUTPUT_FILE, 'w') as f:\n",
    "            json.dump(all_financial_reports, f, indent=2)\n",
    "\n",
    "# === Selesai, Simpan ke JSON dan Tutup Browser ===\n",
    "with open(JSON_OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(all_financial_reports, f, indent=2)\n",
    "\n",
    "print(f\"📂 Saved {len(all_financial_reports)} financial reports to {JSON_OUTPUT_FILE}\")\n",
    "driver.quit()\n",
    "print(f\"Browser closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Ingestion ke MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data from JSON file\n",
    "print(\"Loading data from JSON file...\")\n",
    "try:\n",
    "    with open(\"financial_reports.json\", \"r\") as f:\n",
    "        all_financial_reports = json.load(f)\n",
    "    print(f\"Loaded {len(all_financial_reports)} financial reports from JSON file\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: financial_reports.json not found!\")\n",
    "    all_financial_reports = []\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: Invalid JSON format in financial_reports.json!\")\n",
    "    all_financial_reports = []\n",
    "\n",
    "if all_financial_reports:\n",
    "    # Connect to MongoDB Atlas\n",
    "    connection_string = \"mongodb+srv://kelompok-5:FwJP0h7Bo6cTpEol@big-data.do3of.mongodb.net/?retryWrites=true&w=majority&ssl=true\"\n",
    "    client = pymongo.MongoClient(connection_string, \n",
    "                               maxPoolSize=100,  # Increase connection pool\n",
    "                               retryWrites=True)\n",
    "\n",
    "    # Select database and collection\n",
    "    db = client[\"Big_Data_kel_5\"]  # Database name\n",
    "    collection = db[\"Data_LaporanKeuangan\"]     # Collection name\n",
    "\n",
    "    # Create index for faster lookups if it doesn't exist\n",
    "    collection.create_index([(\"company\", 1)], unique=True, background=True)\n",
    "\n",
    "    # Get all existing company records in one query\n",
    "    print(\"Fetching existing records...\")\n",
    "    existing_companies = set()\n",
    "    for doc in collection.find({}, {\"company\": 1, \"_id\": 0}):\n",
    "        existing_companies.add(doc[\"company\"])\n",
    "\n",
    "    print(f\"Found existing records for {len(existing_companies)} companies\")\n",
    "\n",
    "    # Prepare bulk operations\n",
    "    bulk_ops = []\n",
    "    new_record_count = 0\n",
    "    update_count = 0\n",
    "    batch_size = 20  # Smaller batch size for large documents\n",
    "\n",
    "    print(\"Preparing bulk operations...\")\n",
    "    for record in all_financial_reports:\n",
    "        company = record[\"company\"]\n",
    "        \n",
    "        # Check if this company already exists\n",
    "        if company in existing_companies:\n",
    "            # Update the existing record\n",
    "            bulk_ops.append(\n",
    "                pymongo.UpdateOne(\n",
    "                    {\"company\": company},\n",
    "                    {\"$set\": record}\n",
    "                )\n",
    "            )\n",
    "            update_count += 1\n",
    "        else:\n",
    "            # Insert new record\n",
    "            bulk_ops.append(pymongo.InsertOne(record))\n",
    "            new_record_count += 1\n",
    "            existing_companies.add(company)  # Add to tracking set\n",
    "        \n",
    "        # Execute batch if reached batch size\n",
    "        if len(bulk_ops) >= batch_size:\n",
    "            if bulk_ops:\n",
    "                result = collection.bulk_write(bulk_ops, ordered=False)\n",
    "                print(f\"Processed batch: {result.inserted_count} inserted, {result.modified_count} modified\")\n",
    "                bulk_ops = []\n",
    "\n",
    "    # Insert any remaining operations\n",
    "    if bulk_ops:\n",
    "        result = collection.bulk_write(bulk_ops, ordered=False)\n",
    "        print(f\"Processed final batch: {result.inserted_count} inserted, {result.modified_count} modified\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Completed MongoDB ingestion process. Inserted {new_record_count} new records and updated {update_count} records in {elapsed_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"No data to ingest to MongoDB.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
