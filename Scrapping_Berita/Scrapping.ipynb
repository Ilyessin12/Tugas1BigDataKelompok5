{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_news(emiten_list, output_file):\n",
    "    # Dictionary to store all news data\n",
    "    news_data = []\n",
    "\n",
    "    # Setup WebDriver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    # Loop through each ticker\n",
    "    for emiten in emiten_list:\n",
    "        print(f\"Searching news for {emiten}...\")\n",
    "        \n",
    "        # Open the search URL\n",
    "        search_url = \"http://www.iqplus.info/news/search/\"\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Wait for the search input element to be present and interact with it\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.NAME, \"search\"))\n",
    "            )\n",
    "            # Interact with the search input once it's present\n",
    "            search_input.send_keys(emiten)\n",
    "            search_input.submit()\n",
    "        except:\n",
    "            print(f\"Search input element not found for {emiten}. Moving to next ticker.\")\n",
    "            continue  # Skip to the next ticker if the search input is not found\n",
    "\n",
    "        time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        # Parse the page source\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Find news items\n",
    "        news_list = soup.find_all(\"li\", style=\"text-transform:capitalize;\")\n",
    "\n",
    "        # Extract news details\n",
    "        if news_list:\n",
    "            print(f\"Found {len(news_list)} news items for {emiten}\")\n",
    "            for news in news_list:\n",
    "                date_time = news.find(\"b\").text.strip() if news.find(\"b\") else \"No Date\"\n",
    "                title = news.find(\"a\").text.strip() if news.find(\"a\") else \"No Title\"\n",
    "                link = news.find(\"a\")[\"href\"] if news.find(\"a\") else \"#\"\n",
    "\n",
    "                # Check if title contains the emiten name followed by a colon\n",
    "                if f\"{emiten}:\" in title:\n",
    "                    # Append news as a dictionary\n",
    "                    news_data.append({\n",
    "                        \"Emiten\": emiten,\n",
    "                        \"Date\": date_time,\n",
    "                        \"Title\": title,\n",
    "                        \"Link\": link\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping news item as title does not contain '{emiten}:'\")\n",
    "        else:\n",
    "            print(f\"No news found for {emiten}.\")\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Save the news data to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(news_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"News data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process JSON files from pt1 to pt5\n",
    "for i in range(1, 6):\n",
    "    input_file = f\"emiten_list_pt{i}.json\"\n",
    "    output_file = f\"stock_news.json_pt{i}\"\n",
    "\n",
    "    try:\n",
    "        # Read emiten list from JSON file\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            emiten_list = json.load(file)\n",
    "        print(f\"Successfully loaded {len(emiten_list)} emiten from {input_file}\")\n",
    "\n",
    "            # Scrape news for the current emiten list\n",
    "        scrape_news(emiten_list, output_file)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_file} not found. Skipping...\")\n",
    "        continue\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {input_file}. Skipping...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
