{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_news(emiten_list, output_file):\n",
    "    # Dictionary to store all news data\n",
    "    news_data = []\n",
    "\n",
    "    # Setup WebDriver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "    # Loop through each ticker\n",
    "    for emiten in emiten_list:\n",
    "        print(f\"Searching news for {emiten}...\")\n",
    "        \n",
    "        # Open the search URL\n",
    "        search_url = \"http://www.iqplus.info/news/search/\"\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Wait for the search input element to be present and interact with it\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.NAME, \"search\"))\n",
    "            )\n",
    "            # Interact with the search input once it's present\n",
    "            search_input.send_keys(emiten)\n",
    "            search_input.submit()\n",
    "        except:\n",
    "            print(f\"Search input element not found for {emiten}. Moving to next ticker.\")\n",
    "            continue  # Skip to the next ticker if the search input is not found\n",
    "\n",
    "        time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        # Parse the page source\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Find news items\n",
    "        news_list = soup.find_all(\"li\", style=\"text-transform:capitalize;\")\n",
    "\n",
    "        # Extract news details\n",
    "        if news_list:\n",
    "            print(f\"Found {len(news_list)} news items for {emiten}\")\n",
    "            for news in news_list:\n",
    "                date_time = news.find(\"b\").text.strip() if news.find(\"b\") else \"No Date\"\n",
    "                title = news.find(\"a\").text.strip() if news.find(\"a\") else \"No Title\"\n",
    "                link = news.find(\"a\")[\"href\"] if news.find(\"a\") else \"#\"\n",
    "\n",
    "                # Check if title contains the emiten name followed by a colon\n",
    "                if f\"{emiten}:\" in title:\n",
    "                    # Append news as a dictionary\n",
    "                    news_data.append({\n",
    "                        \"Emiten\": emiten,\n",
    "                        \"Date\": date_time,\n",
    "                        \"Title\": title,\n",
    "                        \"Link\": link\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping news item as title does not contain '{emiten}:'\")\n",
    "        else:\n",
    "            print(f\"No news found for {emiten}.\")\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Save the news data to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(news_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"News data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process JSON files from pt1 to pt5\n",
    "for i in range(1, 6):\n",
    "    input_file = f\"emiten_list_pt{i}.json\"\n",
    "    output_file = f\"stock_news.json_pt{i}\"\n",
    "\n",
    "    try:\n",
    "        # Read emiten list from JSON file\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            emiten_list = json.load(file)\n",
    "        print(f\"Successfully loaded {len(emiten_list)} emiten from {input_file}\")\n",
    "\n",
    "            # Scrape news for the current emiten list\n",
    "        scrape_news(emiten_list, output_file)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_file} not found. Skipping...\")\n",
    "        continue\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {input_file}. Skipping...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Ingestion ke MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "# Start timing\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data from all JSON files\n",
    "print(\"Loading data from JSON files...\")\n",
    "all_news_data = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    file_name = f\"stock_news.json_pt{i}\"\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            news_data = json.load(f)\n",
    "            all_news_data.extend(news_data)\n",
    "            print(f\"Loaded {len(news_data)} records from {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {file_name} not found. Skipping...\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Invalid JSON format in {file_name}. Skipping...\")\n",
    "\n",
    "print(f\"Loaded total of {len(all_news_data)} news records from all files\")\n",
    "\n",
    "# Connect to MongoDB Atlas\n",
    "connection_string = \"mongodb+srv://kelompok-5:FwJP0h7Bo6cTpEol@big-data.do3of.mongodb.net/?retryWrites=true&w=majority&ssl=true\"\n",
    "client = pymongo.MongoClient(connection_string, \n",
    "                           maxPoolSize=100,  # Increase connection pool\n",
    "                           retryWrites=True)\n",
    "\n",
    "# Select database and collection\n",
    "db = client[\"Big_Data_kel_5\"]  # Database name\n",
    "collection = db[\"Stock_News\"]   # Collection name\n",
    "\n",
    "# Create compound index for faster lookups if it doesn't exist\n",
    "collection.create_index([(\"Emiten\", 1), (\"Date\", 1), (\"Title\", 1)], unique=True, background=True)\n",
    "\n",
    "# Get all existing emiten-date-title combinations in one query\n",
    "print(\"Fetching existing records...\")\n",
    "existing_records = {}\n",
    "for doc in collection.find({}, {\"Emiten\": 1, \"Date\": 1, \"Title\": 1, \"_id\": 0}):\n",
    "    emiten = doc[\"Emiten\"]\n",
    "    date = doc[\"Date\"]\n",
    "    title = doc[\"Title\"]\n",
    "    \n",
    "    if emiten not in existing_records:\n",
    "        existing_records[emiten] = {}\n",
    "    \n",
    "    if date not in existing_records[emiten]:\n",
    "        existing_records[emiten][date] = set()\n",
    "    \n",
    "    existing_records[emiten][date].add(title)\n",
    "\n",
    "print(f\"Found existing records for {len(existing_records)} emitens\")\n",
    "\n",
    "# Prepare bulk operations\n",
    "bulk_ops = []\n",
    "new_record_count = 0\n",
    "batch_size = 1000  # Process in batches\n",
    "\n",
    "print(\"Preparing bulk operations...\")\n",
    "for record in all_news_data:\n",
    "    emiten = record[\"Emiten\"]\n",
    "    date = record[\"Date\"]\n",
    "    title = record[\"Title\"]\n",
    "    \n",
    "    # Skip if this record already exists\n",
    "    if (emiten in existing_records and \n",
    "        date in existing_records[emiten] and \n",
    "        title in existing_records[emiten][date]):\n",
    "        continue\n",
    "    \n",
    "    # Add to bulk operations\n",
    "    bulk_ops.append(pymongo.InsertOne(record))\n",
    "    new_record_count += 1\n",
    "    \n",
    "    # Execute batch if reached batch size\n",
    "    if len(bulk_ops) >= batch_size:\n",
    "        if bulk_ops:\n",
    "            collection.bulk_write(bulk_ops, ordered=False)\n",
    "            print(f\"Inserted batch of {len(bulk_ops)} records\")\n",
    "            bulk_ops = []\n",
    "\n",
    "# Insert any remaining operations\n",
    "if bulk_ops:\n",
    "    collection.bulk_write(bulk_ops, ordered=False)\n",
    "    print(f\"Inserted final batch of {len(bulk_ops)} records\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Completed MongoDB ingestion process. Inserted {new_record_count} new records in {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
