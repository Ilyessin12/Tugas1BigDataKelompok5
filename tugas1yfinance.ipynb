{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library untuk mengambil data tabular dari yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "# Library untuk HTTP requests dan parsing HTML\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Library untuk otomatisasi web scraping dengan Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Library untuk parsing file XML\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Library untuk mengekstrak file ZIP (misalnya instance.zip)\n",
    "import zipfile\n",
    "\n",
    "# Library untuk koneksi ke MongoDB\n",
    "import pymongo\n",
    "\n",
    "# Library tambahan (opsional) seperti untuk pengolahan data dan logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Data\n",
    "\n",
    "Setelah import library maka akan dilakukan proses pengambilan data menggunakan yfinance lalu disimpan dalam bentuk json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emiten list from JSON file\n",
    "with open('emiten_list.json', 'r') as file:\n",
    "    emiten_list = json.load(file)\n",
    "\n",
    "# Create a list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Process each emiten\n",
    "for emiten in emiten_list:\n",
    "    print(f\"Processing {emiten}...\")\n",
    "    try:\n",
    "        # Get stock data\n",
    "        stock = yf.Ticker(emiten)\n",
    "        data = stock.history(period=\"1y\")\n",
    "        \n",
    "        # Reset index to make Date a regular column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        # Convert Date to string format\n",
    "        data[\"Date\"] = data[\"Date\"].apply(lambda x: x.strftime(\"%d/%m/%Y - %H:%M\"))\n",
    "        \n",
    "        # Convert DataFrame to list of dictionaries\n",
    "        records = json.loads(data.to_json(orient=\"records\"))\n",
    "        \n",
    "        # Add emiten information to each record\n",
    "        for record in records:\n",
    "            record[\"emiten\"] = emiten\n",
    "            \n",
    "        # Add these records to our main list\n",
    "        all_data.extend(records)\n",
    "        \n",
    "        print(f\"✅ Added {len(records)} records for {emiten}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {emiten}: {str(e)}\")\n",
    "\n",
    "# Save all data to a single JSON file\n",
    "output_file = \"yfinancescrape.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(all_data, f, indent=2)\n",
    "\n",
    "print(f\"Successfully saved {len(all_data)} records to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion ke MongoDB\n",
    "Setelah dibuat JSON filenya, maka langkah selanjutnya adalah memasukkannya ke mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data from JSON file\n",
    "print(\"Loading data from JSON file...\")\n",
    "with open(\"yfinancescrape.json\", \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_data)} records from JSON file\")\n",
    "\n",
    "# Connect to MongoDB Atlas\n",
    "connection_string = \"mongodb+srv://kelompok-5:FwJP0h7Bo6cTpEol@big-data.do3of.mongodb.net/?retryWrites=true&w=majority&ssl=true\"\n",
    "client = pymongo.MongoClient(connection_string, \n",
    "                            maxPoolSize=100,  # Increase connection pool\n",
    "                            retryWrites=True)\n",
    "\n",
    "# Select database and collection\n",
    "db = client[\"Big_Data_kel_5\"]  # Database name\n",
    "collection = db[\"Test_yfinance\"]     # Collection name\n",
    "\n",
    "# Create compound index for faster lookups if it doesn't exist\n",
    "collection.create_index([(\"emiten\", 1), (\"Date\", 1)], unique=True, background=True)\n",
    "\n",
    "# Get all existing emiten-date pairs in one query (much faster than multiple queries)\n",
    "print(\"Fetching existing records...\")\n",
    "existing_records = {}\n",
    "for doc in collection.find({}, {\"emiten\": 1, \"Date\": 1, \"_id\": 0}):\n",
    "    emiten = doc[\"emiten\"]\n",
    "    date = doc[\"Date\"]\n",
    "    if emiten not in existing_records:\n",
    "        existing_records[emiten] = set()\n",
    "    existing_records[emiten].add(date)\n",
    "\n",
    "print(f\"Found existing records for {len(existing_records)} emitens\")\n",
    "\n",
    "# Prepare bulk operations\n",
    "bulk_ops = []\n",
    "new_record_count = 0\n",
    "batch_size = 1000  # Process in batches\n",
    "\n",
    "print(\"Preparing bulk operations...\")\n",
    "for record in all_data:\n",
    "    emiten = record[\"emiten\"]\n",
    "    date = record[\"Date\"]\n",
    "    \n",
    "    # Skip if this record already exists\n",
    "    if emiten in existing_records and date in existing_records[emiten]:\n",
    "        continue\n",
    "    \n",
    "    # Add to bulk operations\n",
    "    bulk_ops.append(pymongo.InsertOne(record))\n",
    "    new_record_count += 1\n",
    "    \n",
    "    # Execute batch if reached batch size\n",
    "    if len(bulk_ops) >= batch_size:\n",
    "        if bulk_ops:\n",
    "            collection.bulk_write(bulk_ops, ordered=False)\n",
    "            print(f\"Inserted batch of {len(bulk_ops)} records\")\n",
    "            bulk_ops = []\n",
    "\n",
    "# Insert any remaining operations\n",
    "if bulk_ops:\n",
    "    collection.bulk_write(bulk_ops, ordered=False)\n",
    "    print(f\"Inserted final batch of {len(bulk_ops)} records\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Completed MongoDB ingestion process. Inserted {new_record_count} new records in {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
